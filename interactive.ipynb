{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example notebook for inspecting the DNC model trained on the repeat copy task.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from dnc import dnc, access\n",
    "from dnc import repeat_copy\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# Update hyper parameters based on trained model\n",
    "flags_dict = {\n",
    "    # Model parameters\n",
    "    \"hidden_size\": 64, # Size of LSTM hidden layer.\n",
    "    \"memory_size\": 16, # The number of memory slots.\n",
    "    \"word_size\": 16, #\"The width of each memory slot.\"\n",
    "    \"num_write_heads\": 1, #\"Number of memory write heads.\"\n",
    "    \"num_read_heads\": 4, #\"Number of memory read heads.\"\n",
    "    \"clip_value\": 20, #\"Maximum absolute value of controller and dnc outputs.\"\n",
    "\n",
    "    # Optimizer parameters.\n",
    "    \"max_grad_norm\": 50, #\"Gradient clipping norm limit.\"\n",
    "    \"learning_rate\": 1e-4, #\"Optimizer learning rate.\"\n",
    "    \"optimizer_epsilon\": 1e-10, #\"Epsilon used for RMSProp optimizer.\"\n",
    "\n",
    "    # Task parameters\n",
    "    \"batch_size\": 1, #\"Batch size for training.\"\n",
    "    \"num_bits\": 8, #\"Dimensionality of each vector to copy\"\n",
    "    \"min_length\": 1,#\"Lower limit on number of vectors in the observation pattern to copy\"\n",
    "    \"max_length\": 3,#\"Upper limit on number of vectors in the observation pattern to copy\"\n",
    "    \"min_repeats\": 1,#\"Lower limit on number of copy repeats.\"\n",
    "    \"max_repeats\": 3, #\"Upper limit on number of copy repeats.\"\n",
    "\n",
    "    \"checkpoint_dir\": \"./logs/repeat_copy/checkpoint\", #\"Checkpointing directory.\"\n",
    "}\n",
    "\n",
    "flags_schema = namedtuple('flags_schema', list(flags_dict.keys()))\n",
    "FLAGS = flags_schema(**flags_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Load dnc core model from checkpoint directory\"\"\"\n",
    "    access_config = {\n",
    "        \"memory_size\": FLAGS.memory_size,\n",
    "        \"word_size\": FLAGS.word_size,\n",
    "        \"num_reads\": FLAGS.num_read_heads,\n",
    "        \"num_writes\": FLAGS.num_write_heads,\n",
    "    }\n",
    "    controller_config = {\n",
    "        #\"hidden_size\": FLAGS.hidden_size,\n",
    "        \"units\": FLAGS.hidden_size,\n",
    "    }\n",
    "    clip_value = FLAGS.clip_value\n",
    "\n",
    "    dnc_cell = dnc.DNC(\n",
    "        access_config, controller_config, FLAGS.num_bits + 1, FLAGS.batch_size, clip_value)\n",
    "    dnc_core = tf.keras.layers.RNN(\n",
    "        cell=dnc_cell,\n",
    "        time_major=True,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "    )\n",
    "    optimizer = tf.compat.v1.train.RMSPropOptimizer(\n",
    "        FLAGS.learning_rate, epsilon=FLAGS.optimizer_epsilon)\n",
    "\n",
    "    # Set up model checkpointing\n",
    "    checkpoint = tf.train.Checkpoint(model=dnc_core, optimizer=optimizer)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, FLAGS.checkpoint_dir, max_to_keep=10)\n",
    "\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "    return dnc_core\n",
    "\n",
    "\n",
    "dnc_core = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weights_from_dnc_state(dnc_state):\n",
    "    return dnc_state[dnc.ACCESS_STATE][access.READ_WEIGHTS]\n",
    "def write_weights_from_dnc_state(dnc_state):\n",
    "    return dnc_state[dnc.ACCESS_STATE][access.WRITE_WEIGHTS]\n",
    "def memory_from_dnc_state(dnc_state):\n",
    "    return dnc_state[dnc.ACCESS_STATE][access.MEMORY]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    x,\n",
    "    mask,\n",
    "    rnn_model,\n",
    "):\n",
    "    \"\"\"Obtain output sequence and intermediate states when evaluating x.\n",
    "    \n",
    "    Args:\n",
    "        x: input tensor\n",
    "        mask: Mask tensor, currently unused\n",
    "        rnn_model: keras.layers.RNN instance\n",
    "        \n",
    "    Returns:\n",
    "        output_sequence: List of tensors representing the model output\n",
    "            sequence for each time step\n",
    "        output_states: List of rnn states (may be nested list of tensors)\n",
    "            output for each time step\n",
    "    \"\"\"\n",
    "    output_sequence = []\n",
    "    output_states = []\n",
    "    input_state = rnn_model.get_initial_state(inputs=x)\n",
    "    \n",
    "    for input_seq in x:\n",
    "        output = rnn_model(\n",
    "            inputs=tf.expand_dims(input_seq, axis=0),\n",
    "            initial_state=input_state,\n",
    "        )\n",
    "        \n",
    "        output_seq = output[0]\n",
    "        output_state = output[1:]\n",
    "        \n",
    "        output_sequence.append(tf.round(tf.sigmoid(output_seq)))\n",
    "        #output_sequence.append(output_seq)\n",
    "        output_states.append(output_state)\n",
    "\n",
    "        input_state = output_state\n",
    "        \n",
    "    return output_sequence, output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29aad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_results(obs, targ, pred, mask):\n",
    "    obs = tf.transpose(obs)\n",
    "    targ = tf.transpose(targ)\n",
    "    pred = tf.transpose(tf.squeeze(pred))\n",
    "    \n",
    "    seaborn.set(rc = {'figure.figsize':(\n",
    "        15.0 / 64 * obs.shape[1], # time, x-axis\n",
    "        15.0 / 64 * obs.shape[0], # biz position, y-axis\n",
    "    )})\n",
    "    \n",
    "    seaborn.heatmap(obs)\n",
    "    plt.title('RepeatCopy Task Inputs')\n",
    "    plt.xlabel('time step')\n",
    "    plt.ylabel('bit position')\n",
    "    plt.show()\n",
    "    \n",
    "    seaborn.heatmap(targ)\n",
    "    plt.title('RepeatCopy Task Target')\n",
    "    plt.xlabel('time step')\n",
    "    plt.ylabel('bit position')\n",
    "    plt.show()\n",
    "    \n",
    "    seaborn.heatmap(pred)\n",
    "    plt.title('RepeatCopy Task Model Outputs')\n",
    "    plt.xlabel('time step')\n",
    "    plt.ylabel('bit position')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_states(states):\n",
    "    #memory = [memory_from_dnc_state(state)[0] for state in states]\n",
    "    read_weights = [read_weights_from_dnc_state(state)[0] for state in states]\n",
    "    read_weights = tf.transpose(tf.stack(read_weights), [1,2,0])\n",
    "    \n",
    "    write_weights = [write_weights_from_dnc_state(state)[0] for state in states]\n",
    "    write_weights = tf.transpose(tf.stack(write_weights), [1,2,0])\n",
    "        \n",
    "    \"\"\"memory_color_range = {\n",
    "        'vmin': np.min(memory),\n",
    "        'vmax': np.max(memory)\n",
    "    }\"\"\"\n",
    "    read_weights_color_range = {\n",
    "        'vmin': np.min(read_weights),\n",
    "        'vmax': np.max(read_weights),\n",
    "    }\n",
    "    write_weights_color_range = {\n",
    "        'vmin': np.min(write_weights),\n",
    "        'vmax': np.max(write_weights),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    seaborn.set(rc = {'figure.figsize':(\n",
    "        15.0 / 64 * write_weights.shape[2], # time, x-axis\n",
    "        15.0 / 64 * write_weights.shape[1], # memory, y-axis\n",
    "    )})\n",
    "    \n",
    "    # Visualize write weights over time\n",
    "    for i, write_head in enumerate(write_weights):\n",
    "        seaborn.heatmap(write_head, **write_weights_color_range)\n",
    "        plt.title(f'Write Weights for Write Head {i}')\n",
    "        plt.xlabel('time step')\n",
    "        plt.ylabel('memory slot')\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualize read weights over time\n",
    "    for i, read_head in enumerate(read_weights):\n",
    "        seaborn.heatmap(read_head, **read_weights_color_range)\n",
    "        plt.title(f'Read Weights for Read Head {i}')\n",
    "        plt.xlabel('time step')\n",
    "        plt.ylabel('memory slot')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89115c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_model(x, num_repeats):\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    obs, targ, mask = repeat_copy.RepeatCopy.derive_data_from_inputs(\n",
    "        x, \n",
    "        num_repeats, \n",
    "        10 # repeat_copy._norm_max, default value of 10, modify if using different norm\n",
    "    )\n",
    "    \n",
    "    output_sequence, states = evaluate_model(tf.expand_dims(obs, [1]), None, dnc_core)\n",
    "    \n",
    "    visualize_results(obs, targ, tf.stack(output_sequence), mask)\n",
    "    visualize_states(states)\n",
    "    return output_sequence, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb76634",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = debug_model([[1]*8, [0]*8], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f6272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
